apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: prow
    role: alert-rules
  name: prometheus-prow-rules
  namespace: prow-monitoring
spec:
  groups:
  - name: prow-monitoring-absent
    rules:
    - alert: ServiceLost
      annotations:
        message: The service {{ $labels.job }} has at most 0 instance for 5 minutes.
      expr: |
        sum(up{job=~"prometheus|alertmanager"}) by (job) <= 0
      for: 5m
      labels:
        severity: critical
        slo: monitoring
    - alert: alertmanagerDown
      annotations:
        message: The service alertmanager has been down for 5 minutes.
      expr: |
        absent(up{job="alertmanager"} == 1)
      for: 5m
      labels:
        severity: critical
        slo: monitoring
    - alert: prometheusDown
      annotations:
        message: The service prometheus has been down for 5 minutes.
      expr: |
        absent(up{job="prometheus"} == 1)
      for: 5m
      labels:
        severity: critical
        slo: monitoring
    - alert: grafanaDown
      annotations:
        message: The service grafana has been down for 5 minutes.
      expr: |
        absent(up{job="grafana"} == 1)
      for: 5m
      labels:
        severity: critical
        slo: monitoring
  - name: configmap-full
    rules:
    - alert: ConfigMapFullInOneWeek
      annotations:
        message: Based on recent sampling, the ConfigMap {{ $labels.name }} in Namespace
          {{ $labels.namespace }} is expected to fill up within a week. Currently
          {{ printf "%0.2f" $value }}% is available.
      expr: |
        100 * ((1048576 - prow_configmap_size_bytes) / 1048576) < 15
        and
        predict_linear(prow_configmap_size_bytes[24h], 7 * 24 * 3600) > 1048576
      for: 5m
      labels:
        severity: high
  - name: ghproxy
    rules:
    - alert: ghproxy-specific-status-code-5xx
      annotations:
        message: '{{ $value | humanize }}% of all requests for {{ $labels.path }}
          through the GitHub proxy are erroring with code {{ $labels.status }}. Check
          <https://monitoring.falco.org/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&panelId=9|the
          ghproxy dashboard>.'
      expr: |
        sum(rate(github_request_duration_count{status=~"5.."}[5m])) by (status,path) / ignoring(status) group_left sum(rate(github_request_duration_count[5m])) by (path) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-global-status-code-5xx
      annotations:
        message: '{{ $value | humanize }}% of all API requests through the GitHub
          proxy are errorring with code {{ $labels.status }}. Check <https://monitoring.falco.org/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&panelId=8|the
          ghproxy dashboard>.'
      expr: |
        sum(rate(github_request_duration_count{status=~"5.."}[5m])) by (status) / ignoring(status) group_left sum(rate(github_request_duration_count[5m])) * 100 > 3
      labels:
        severity: warning
    - alert: ghproxy-specific-status-code-4xx
      annotations:
        message: '{{ $value | humanize }}% of all requests for {{ $labels.path }}
          through the GitHub proxy are erroring with code {{ $labels.status }}. Check
          <https://monitoring.falco.org/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&panelId=9|the
          ghproxy dashboard>.'
      expr: |
        sum by(status, path) (rate(github_request_duration_count{status!="404",status!="410",status=~"4..",path!="/repos/:owner/:repo/pulls/:pullId/requested_reviewers",path!="/search/issues",path!="/repos/:owner/:repo/pulls/:pullId/merge",path!="/repos/:owner/:repo/statuses/:statusId"}[30m])) / ignoring(status) group_left() sum by(path) (rate(github_request_duration_count[30m])) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-specific-status-code-not-422
      annotations:
        message: '{{ $value | humanize }}% of all requests for {{ $labels.path }}
          through the GitHub proxy are erroring with code {{ $labels.status }}. Check
          <https://monitoring.falco.org/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&panelId=9|the
          ghproxy dashboard>.'
      expr: |
        sum by(status, path) (rate(github_request_duration_count{status!="404",status!="410", status!="422", status=~"4..",path=~"/repos/:owner/:repo/pulls/:pullId/requested_reviewers|/repos/:owner/:repo/statuses/:statusId"}[30m])) / ignoring(status) group_left() sum by(path) (rate(github_request_duration_count[30m])) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-specific-status-code-not-403
      annotations:
        message: '{{ $value | humanize }}% of all requests for {{ $labels.path }}
          through the GitHub proxy are erroring with code {{ $labels.status }}. Check
          <https://monitoring.falco.org/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&panelId=9|the
          ghproxy dashboard>.'
      expr: |
        sum by(status, path) (rate(github_request_duration_count{status!="404",status!="410", status!="403", status=~"4..",path="/search/issues"}[30m])) / ignoring(status) group_left() sum by(path) (rate(github_request_duration_count[30m])) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-specific-status-code-not-405
      annotations:
        message: '{{ $value | humanize }}% of all requests for {{ $labels.path }}
          through the GitHub proxy are erroring with code {{ $labels.status }}. Check
          <https://monitoring.falco.org/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&panelId=9|the
          ghproxy dashboard>.'
      expr: |
        sum by(status, path) (rate(github_request_duration_count{status!="404",status!="410", status!="405", status=~"4..",path="/repos/:owner/:repo/pulls/:pullId/merge"}[30m])) / ignoring(status) group_left() sum by(path) (rate(github_request_duration_count[30m])) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-global-status-code-4xx
      annotations:
        message: '{{ $value | humanize }}% of all API requests through the GitHub
          proxy are errorring with code {{ $labels.status }}. Check <https://monitoring.falco.org/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&panelId=8|the
          ghproxy dashboard>.'
      expr: |
        sum(rate(github_request_duration_count{status=~"4..",status!="404",status!="410",status!="403",status!="405",status!="422"}[30m])) by (status) / ignoring(status) group_left sum(rate(github_request_duration_count[30m])) * 100 > 3
      labels:
        severity: warning
    - alert: ghproxy-global-status-code-403-405-422
      annotations:
        message: '{{ $value | humanize }}% of all API requests through the GitHub
          proxy are errorring with code {{ $labels.status }}. Check <https://monitoring.falco.org/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&panelId=8|the
          ghproxy dashboard>.'
      expr: |
        sum(rate(github_request_duration_count{status=~"403|405|422"}[30m])) by (status) / ignoring(status) group_left sum(rate(github_request_duration_count[30m])) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-running-out-github-tokens-in-a-hour
      annotations:
        message: token {{ $labels.token_hash }} will run out of API quota before the
          next reset.
      expr: |
        github_token_usage{job="ghproxy"} <  1500
        and
        predict_linear(github_token_usage{job="ghproxy"}[30m], 1 * 3600) < 0
      for: 5m
      labels:
        severity: high
  - name: abnormal webhook behaviors
    rules:
    - alert: no-webhook-calls
      annotations:
        message: There have been no webhook calls for 1 hour for 1h
      expr: |
        (sum(increase(prow_webhook_counter[1h])) == 0 or absent(prow_webhook_counter))
        and ((day_of_week() > 0) and (day_of_week() < 6) and (hour() >= 16))
      for: 1h
      labels:
        severity: high
        slo: hook
  - name: sinker-missing
    rules:
    - alert: SinkerNotRemovingPods
      annotations:
        message: Sinker has not removed any Pods in the last 3 hours, likely indicating
          an outage in the service.
      expr: |
        absent(sum(rate(sinker_pods_removed[3h]))) == 1
      for: 5m
      labels:
        severity: high
        slo: sinker
    - alert: SinkerNotRemovingProwJobs
      annotations:
        message: Sinker has not removed any Prow jobs in the last 3 hours, likely
          indicating an outage in the service.
      expr: |
        absent(sum(rate(sinker_prow_jobs_cleaned[3h]))) == 1
      for: 5m
      labels:
        severity: high
        slo: sinker
  - name: Tide progress
    rules:
    - alert: Sync controller heartbeat
      annotations:
        message: The Tide "sync" controller has not synced in 15 minutes. See the
          <https://monitoring.falco.org/d/d69a91f76d8110d3e72885ee5ce8038e/tide-dashboard?orgId=1&from=now-24h&to=now&fullscreen&panelId=7|processing
          time graph>.
      expr: |
        sum(increase(tidesyncheartbeat{controller="sync"}[15m])) < 1
      for: 5m
      labels:
        severity: critical
        slo: tide
    - alert: Status-update controller heartbeat
      annotations:
        message: The Tide "status-update" controller has not synced in 30 minutes.
          See the <https://monitoring.falco.org/d/d69a91f76d8110d3e72885ee5ce8038e/tide-dashboard?orgId=1&from=now-24h&to=now&fullscreen&panelId=7|processing
          time graph>.
      expr: |
        sum(increase(tidesyncheartbeat{controller="status-update"}[30m])) < 1
      for: 5m
      labels:
        severity: critical
        slo: tide
    - alert: 'TidePool error rate: individual'
      annotations:
        message: At least one Tide pool encountered 3+ sync errors in a 10 minute
          window. If the TidePoolErrorRateMultiple alert has not fired this is likely
          an isolated configuration issue. See the <https://prow.falco.org/tide-history|/tide-history>
          page and the <https://monitoring.falco.org/d/d69a91f76d8110d3e72885ee5ce8038e/tide-dashboard?orgId=1&fullscreen&panelId=6&from=now-24h&to=now|sync
          error graph>.
      expr: |
        ((sum(increase(tidepoolerrors{org!="kubeflow"}[10m])) by (org, repo, branch)) or vector(0)) >= 3
      for: 5m
      labels:
        severity: warning
    - alert: 'TidePool error rate: multiple'
      annotations:
        message: Tide encountered 3+ sync errors in a 10 minute window in at least
          3 different repos that it handles. See the <https://prow.falco.org/tide-history|/tide-history>
          page and the <https://monitoring.falco.org/d/d69a91f76d8110d3e72885ee5ce8038e/tide-dashboard?orgId=1&fullscreen&panelId=6&from=now-24h&to=now|sync
          error graph>.
      expr: |
        (count(sum(increase(tidepoolerrors[10m])) by (org, repo) >= 3) or vector(0)) >= 3
      for: 5m
      labels:
        severity: critical
        slo: tide
  - name: Blackbox Prober
    rules:
    - alert: 'Site unavailable: https://prow.falco.org'
      annotations:
        message: 'The blackbox_exporter HTTP probe has detected that the following
          site has been unhealthy (not 2xx HTTP response) for at least 2 minutes:
          <https://prow.falco.org|https://prow.falco.org>.'
      expr: |
        min(probe_success{instance="https://prow.falco.org"}) == 0
      for: 2m
      labels:
        severity: critical
        slo: deck
    - alert: 'Site unavailable: https://download.falco.org'
      annotations:
        message: 'The blackbox_exporter HTTP probe has detected that the following
          site has been unhealthy (not 2xx HTTP response) for at least 2 minutes:
          <https://download.falco.org|https://download.falco.org>.'
      expr: |
        min(probe_success{instance="https://download.falco.org"}) == 0
      for: 2m
      labels:
        severity: critical
        slo: monitoring
    - alert: 'Site unavailable: https://falco.org'
      annotations:
        message: 'The blackbox_exporter HTTP probe has detected that the following
          site has been unhealthy (not 2xx HTTP response) for at least 2 minutes:
          <https://falco.org|https://falco.org>.'
      expr: |
        min(probe_success{instance="https://falco.org"}) == 0
      for: 2m
      labels:
        severity: critical
  - name: Heartbeat ProwJobs
    rules:
    - alert: 'No recent successful runs: `ci-test-infra-prow-checkconfig`'
      annotations:
        message: '@test-infra-oncall The heartbeat job `ci-test-infra-prow-checkconfig`
          has not had a successful run in the past 2h (should run every 30m).'
      expr: |
        sum(increase(prowjob_state_transitions{job_name="ci-test-infra-prow-checkconfig", state="success"}[2h])) < 0.5
      labels:
        severity: critical
        slo: plank
  - interval: 1m
    name: SLO Compliance
    rules:
    - expr: |
        min((absent(ALERTS{alertstate="firing", slo="deck"}) or absent(ALERTS{alertstate="firing", slo="hook"}) or absent(ALERTS{alertstate="firing", slo="prow-controller-manager"}) or absent(ALERTS{alertstate="firing", slo="sinker"}) or absent(ALERTS{alertstate="firing", slo="tide"}) or absent(ALERTS{alertstate="firing", slo="monitoring"})) or (ALERTS{alertstate="firing", slo=~"deck|hook|prow-controller-manager|sinker|tide|monitoring"} - 1)) without (alertstate)
      record: slo_component_ok
    - expr: (vector(1) unless min(slo_component_ok == 0)) or (slo_component_ok ==
        0)
      record: slo_prow_ok
  - name: prow
    rules:
    - alert: prow-pod-crashlooping
      annotations:
        message: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container}})
          is restarting {{ printf "%.2f" $value }} times / 5 minutes.
      expr: rate(kube_pod_container_status_restarts_total{namespace=~"default|prow-monitoring",job="kube-state-metrics"}[5m])
        * 60 * 5 > 0
      for: 1m
      labels:
        severity: critical
